{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DistilBERT fine-tuning으로 감정 분석 모델 학습하기\n",
    "\n",
    "이번 실습에서는 pre-trained된 DistilBERT를 불러와 이전 주차 실습에서 사용하던 감정 분석 문제에 적용합니다. 먼저 필요한 library들을 불러옵니다."
   ],
   "metadata": {
    "id": "sbgz49PvHhLt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip -q install tqdm boto3 requests regex sentencepiece sacremoses datasets"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LqgujQUbv6X",
    "outputId": "f8faaad2-ac05-401b-c8f1-45ec07cbb71b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "그 후, 우리가 사용하는 DistilBERT pre-training 때 사용한 tokenizer를 불러옵니다."
   ],
   "metadata": {
    "id": "6YP3FxG9IF7O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lGiZUoPby6e",
    "outputId": "592b19ee-1892-4821-fcef-24fe40cc185f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DistilBERT의 tokenizer를 불러왔으면 이제 `collate_fn`과 data loader를 정의합니다. 이 과정은 이전 실습과 동일하게 다음과 같이 구현할 수 있습니다."
   ],
   "metadata": {
    "id": "Cvfl_uFLIMWO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "ds = load_dataset(\"fancyzhx/ag_news\")\n",
    "# ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def collate_fn(batch):\n",
    "  texts = [row['text'] for row in batch]\n",
    "  labels = [row['label'] for row in batch]\n",
    "\n",
    "  encoded = tokenizer(texts, padding=True, truncation=False, return_tensors='pt')\n",
    "  texts = encoded['input_ids']\n",
    "  masks = encoded['attention_mask']\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, masks, labels\n",
    "\n",
    "\n",
    "ds = load_dataset(\"fancyzhx/ag_news\")\n",
    "# BATCH_SIZE = len(ds['train'])\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(\n",
    "    ds['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds['test'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "# train_loader = DataLoader(\n",
    "#     Subset(ds['train'],range(0,32)), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    "# )\n",
    "# test_loader = DataLoader(\n",
    "#     Subset(ds['test'], range(0,32)), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {
    "id": "rE-y8sY9HuwP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 pre-trained DistilBERT를 불러옵니다. 이번에는 PyTorch hub에서 제공하는 DistilBERT를 불러봅시다."
   ],
   "metadata": {
    "id": "bF34XkoYIeEm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "model"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJaUp2Vob0U-",
    "outputId": "4cabca2b-06ce-480c-d52a-1381a955464b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "출력 결과를 통해 우리는 DistilBERT의 architecture는 일반적인 Transformer와 동일한 것을 알 수 있습니다.\n",
    "Embedding layer로 시작해서 여러 layer의 Attention, FFN를 거칩니다.\n",
    "\n",
    "이제 DistilBERT를 거치고 난 `[CLS]` token의 representation을 가지고 text 분류를 하는 모델을 구현합시다."
   ],
   "metadata": {
    "id": "uh-tqY8WInQt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "    self.classifier = nn.Linear(self.encoder.config.dim, 4)\n",
    "    self.dropout = nn.Dropout(self.encoder.config.seq_classif_dropout)\n",
    "    for param in self.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "      \n",
    "  def forward(self, input_ids, attention_mask=None):\n",
    "    # print(attention_mask)\n",
    "    x = self.encoder(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "    # cls_embedding = outputs.last_hidden_state[:, 0, :]  # Get the [CLS] token's output (batch_size, hidden_size)\n",
    "    # x = self.dropout(outputs)\n",
    "    logits = self.classifier(x[:,0])\n",
    "    return logits\n",
    "        \n",
    "model = TextClassifier()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /Users/heekyung/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xW7ETZQzzNp2",
    "outputId": "acae0d36-0b4a-4c7c-a0cd-5171e7158cf2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "위와 같이 `TextClassifier`의 `encoder`를 불러온 DistilBERT, 그리고 `classifier`를 linear layer로 설정합니다.\n",
    "그리고 `forward` 함수에서 순차적으로 사용하여 예측 결과를 반환합니다.\n",
    "\n",
    "다음은 마지막 classifier layer를 제외한 나머지 부분을 freeze하는 코드를 구현합니다."
   ],
   "metadata": {
    "id": "_hFvSis0JLju"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for param in model.encoder.parameters():\n",
    "  param.requires_grad = False"
   ],
   "outputs": [],
   "metadata": {
    "id": "uyTciaPZ0KYo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "위의 코드는 `encoder`에 해당하는 parameter들의 `requires_grad`를 `False`로 설정하는 모습입니다.\n",
    "`requires_grad`를 `False`로 두는 경우, gradient 계산 및 업데이트가 이루어지지 않아 결과적으로 학습이 되지 않습니다.\n",
    "즉, 마지막 `classifier`에 해당하는 linear layer만 학습이 이루어집니다.\n",
    "이런 식으로 특정 부분들을 freeze하게 되면 효율적으로 학습을 할 수 있습니다.\n",
    "\n",
    "마지막으로 이전과 같은 코드를 사용하여 학습 결과를 확인해봅시다."
   ],
   "metadata": {
    "id": "hU7BWEbgJeKm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import itertools\n",
    "from datetime import datetime\n",
    "from torch.optim import Adam\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "# lr = 3e-5\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  total_loss = 0.\n",
    "  model.train()\n",
    "  for (inputs, masks, labels) in itertools.islice(train_loader, 100):\n",
    "    model.zero_grad()\n",
    "    inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device).float()\n",
    "    preds = model(inputs, masks)\n",
    "    # preds = model(inputs, masks)[..., 0] # loss over 70,000\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch   0 | Train Loss: 78.12759348750114\n",
      "Epoch   1 | Train Loss: 43.71672946214676\n",
      "Epoch   2 | Train Loss: 40.57450167834759\n",
      "Epoch   3 | Train Loss: 38.12422529608011\n",
      "Epoch   4 | Train Loss: 36.38934251666069\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for (inputs, masks, labels) in dataloader:\n",
    "    inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "    preds = model(inputs, \n",
    "                  \n",
    "                  \n",
    "                  masks)\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "    # preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  train_acc = accuracy(model, train_loader)\n",
    "  test_acc = accuracy(model, test_loader)\n",
    "  print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=========> Train acc: 0.886 | Test acc: 0.888\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjphVwXL00E2",
    "outputId": "7526ec71-f015-4f26-8035-3091ed71869e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss가 잘 떨어지고, 이전에 우리가 구현한 Transformer보다 더 빨리 수렴하는 것을 알 수 있습니다."
   ],
   "metadata": {
    "id": "rfFUkEM1ZWeG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ds = load_dataset(\"fancyzhx/ag_news\")\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#   texts = [rows['text'] for rows in batch]\n",
    "#   texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=False ).input_ids)\n",
    "#   return texts, labels\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     ds['train'], batch_size=len(ds['train']), shuffle=True, collate_fn=collate_fn\n",
    "# )\n",
    "\n",
    "# for idx, (inputs, labels) in enumerate(train_loader):\n",
    "#     print(idx, inputs.shape)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Foks5u95ZQ1_"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}