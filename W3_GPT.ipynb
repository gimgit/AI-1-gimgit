{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbgz49PvHhLt"
   },
   "source": [
    "# DistilBERT fine-tuning으로 감정 분석 모델 학습하기\n",
    "\n",
    "이번 실습에서는 pre-trained된 DistilBERT를 불러와 이전 주차 실습에서 사용하던 감정 분석 문제에 적용합니다. 먼저 필요한 library들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LqgujQUbv6X",
    "outputId": "f8faaad2-ac05-401b-c8f1-45ec07cbb71b"
   },
   "outputs": [],
   "source": [
    "!pip -q install tqdm boto3 requests regex sentencepiece sacremoses datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YP3FxG9IF7O"
   },
   "source": [
    "그 후, 우리가 사용하는 DistilBERT pre-training 때 사용한 tokenizer를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lGiZUoPby6e",
    "outputId": "592b19ee-1892-4821-fcef-24fe40cc185f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d792f36baf284e9ebaac0f81ebb95d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1626f9a45746889f5381fb7a0f4804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b79d0dd4a454188877288c04761f556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a7fe9221484fcbb0b40fcf58dd4386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3c8ae2ea574099b265143b7ce51d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai-gpt', clean_up_tokenization_spaces=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cvfl_uFLIMWO"
   },
   "source": [
    "DistilBERT의 tokenizer를 불러왔으면 이제 `collate_fn`과 data loader를 정의합니다. 이 과정은 이전 실습과 동일하게 다음과 같이 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rE-y8sY9HuwP"
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"fancyzhx/ag_news\")\n",
    "# ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def collate_fn(batch):\n",
    "  texts = [row['text'] for row in batch]\n",
    "  labels = [row['label'] for row in batch]\n",
    "\n",
    "  encoded = tokenizer(texts, padding=True, truncation=False, return_tensors='pt')\n",
    "  texts = encoded['input_ids']\n",
    "  masks = encoded['attention_mask']\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, masks, labels\n",
    "\n",
    "\n",
    "ds = load_dataset(\"fancyzhx/ag_news\")\n",
    "# BATCH_SIZE = len(ds['train'])\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(\n",
    "    ds['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds['test'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "# train_loader = DataLoader(\n",
    "#     Subset(ds['train'],range(0,32)), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    "# )\n",
    "# test_loader = DataLoader(\n",
    "#     Subset(ds['test'], range(0,32)), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    "# )\n",
    "# for inputs, masks, labels in train_loader:\n",
    "#     print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF34XkoYIeEm"
   },
   "source": [
    "이제 pre-trained DistilBERT를 불러옵니다. 이번에는 PyTorch hub에서 제공하는 DistilBERT를 불러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJaUp2Vob0U-",
    "outputId": "4cabca2b-06ce-480c-d52a-1381a955464b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/heekyung/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c612b2831054312b8b899f84c2a4c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "OpenAIGPTConfig {\n",
       "  \"_name_or_path\": \"openai-gpt\",\n",
       "  \"afn\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"OpenAIGPTLMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"openai-gpt\",\n",
       "  \"n_ctx\": 512,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 512,\n",
       "  \"n_special\": 0,\n",
       "  \"predict_special_tokens\": true,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.44.1\",\n",
       "  \"vocab_size\": 40478\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'openai-gpt')\n",
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh-tqY8WInQt"
   },
   "source": [
    "출력 결과를 통해 우리는 DistilBERT의 architecture는 일반적인 Transformer와 동일한 것을 알 수 있습니다.\n",
    "Embedding layer로 시작해서 여러 layer의 Attention, FFN를 거칩니다.\n",
    "\n",
    "이제 DistilBERT를 거치고 난 `[CLS]` token의 representation을 가지고 text 분류를 하는 모델을 구현합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xW7ETZQzzNp2",
    "outputId": "acae0d36-0b4a-4c7c-a0cd-5171e7158cf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/heekyung/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'openai-gpt')\n",
    "\n",
    "    self.classifier = nn.Linear(self.encoder.config.n_embd, 4)\n",
    "    self.dropout = nn.Dropout(self.encoder.config.embd_pdrop)\n",
    "    for param in self.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "      \n",
    "  def forward(self, input_ids, attention_mask=None):\n",
    "    x = self.encoder(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "    logits = self.classifier(x[:,-1])\n",
    "    # cls_embedding = outputs.last_hidden_state[:, 0, :]  # Get the [CLS] token's output (batch_size, hidden_size)\n",
    "    # x = self.dropout(outputs)\n",
    "    return logits\n",
    "        \n",
    "model = TextClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hFvSis0JLju"
   },
   "source": [
    "위와 같이 `TextClassifier`의 `encoder`를 불러온 DistilBERT, 그리고 `classifier`를 linear layer로 설정합니다.\n",
    "그리고 `forward` 함수에서 순차적으로 사용하여 예측 결과를 반환합니다.\n",
    "\n",
    "다음은 마지막 classifier layer를 제외한 나머지 부분을 freeze하는 코드를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyTciaPZ0KYo"
   },
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU7BWEbgJeKm"
   },
   "source": [
    "위의 코드는 `encoder`에 해당하는 parameter들의 `requires_grad`를 `False`로 설정하는 모습입니다.\n",
    "`requires_grad`를 `False`로 두는 경우, gradient 계산 및 업데이트가 이루어지지 않아 결과적으로 학습이 되지 않습니다.\n",
    "즉, 마지막 `classifier`에 해당하는 linear layer만 학습이 이루어집니다.\n",
    "이런 식으로 특정 부분들을 freeze하게 되면 효율적으로 학습을 할 수 있습니다.\n",
    "\n",
    "마지막으로 이전과 같은 코드를 사용하여 학습 결과를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 78.746238976717\n",
      "Epoch   1 | Train Loss: 53.24991285800934\n",
      "Epoch   2 | Train Loss: 49.69931522011757\n",
      "Epoch   3 | Train Loss: 46.91623172163963\n",
      "Epoch   4 | Train Loss: 43.74498933553696\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from datetime import datetime\n",
    "from torch.optim import Adam\n",
    "\n",
    "lr = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  total_loss = 0.\n",
    "  model.train()\n",
    "  for (inputs, masks, labels) in itertools.islice(train_loader, 100):\n",
    "    model.zero_grad()\n",
    "    inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device).float()\n",
    "    preds = model(inputs, masks)\n",
    "    # preds = model(inputs, masks)[..., 0] # loss over 70,000\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjphVwXL00E2",
    "outputId": "7526ec71-f015-4f26-8035-3091ed71869e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========> Train acc: 0.852 | Test acc: 0.846\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for (inputs, masks, labels) in dataloader:\n",
    "    inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "    preds = model(inputs, \n",
    "                  \n",
    "                  \n",
    "                  masks)\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "    # preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  train_acc = accuracy(model, train_loader)\n",
    "  test_acc = accuracy(model, test_loader)\n",
    "  print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfFUkEM1ZWeG"
   },
   "source": [
    "Loss가 잘 떨어지고, 이전에 우리가 구현한 Transformer보다 더 빨리 수렴하는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Foks5u95ZQ1_"
   },
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"fancyzhx/ag_news\")\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#   texts = [rows['text'] for rows in batch]\n",
    "#   texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=False ).input_ids)\n",
    "#   return texts, labels\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     ds['train'], batch_size=len(ds['train']), shuffle=True, collate_fn=collate_fn\n",
    "# )\n",
    "\n",
    "# for idx, (inputs, labels) in enumerate(train_loader):\n",
    "#     print(idx, inputs.shape)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
